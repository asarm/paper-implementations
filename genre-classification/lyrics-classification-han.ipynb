{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4840139,"sourceType":"datasetVersion","datasetId":2805070},{"sourceId":6990241,"sourceType":"datasetVersion","datasetId":4017729}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom tqdm import tqdm\nfrom sklearn.utils import resample\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\nimport re\n\nimport nltk\nnltk.download('wordnet')\n\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n    \nfrom nltk.corpus import wordnet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T10:14:41.385426Z","iopub.execute_input":"2023-11-20T10:14:41.385953Z","iopub.status.idle":"2023-11-20T10:14:41.647250Z","shell.execute_reply.started":"2023-11-20T10:14:41.385918Z","shell.execute_reply":"2023-11-20T10:14:41.645515Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"/kaggle/input/genius-song-lyrics-with-language-information/song_lyrics.csv\n/kaggle/input/lyrics/lyrics_data.csv\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package wordnet to /kaggle/working/...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /kaggle/working/corpora/wordnet.zip\n","output_type":"stream"},{"name":"stderr","text":"replace /kaggle/working/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename:  NULL\n(EOF or read error, treating as \"[N]one\" ...)\n","output_type":"stream"}]},{"cell_type":"code","source":"data_path = \"/kaggle/input/genius-song-lyrics-with-language-information/song_lyrics.csv\"\nchunks = pd.read_csv(data_path, iterator=True, chunksize=1000)\ndesired_size = 3500\n\nnew_data = pd.DataFrame()\nfor data in tqdm(chunks):\n    data_filtered = data[data.language == \"en\"][[\"title\", \"artist\", \"tag\", \"lyrics\"]]\n    \n    new_data = pd.concat([new_data, data_filtered])\n    \n    # limit the maximum examples because of the computational power limit\n    if min(new_data.tag.value_counts()) >= desired_size:\n        break\n\n\nundersampled_data = pd.DataFrame()\n\nfor tag in set(new_data.tag):\n    class_df = new_data[new_data.tag == tag]\n    \n    undersampled = resample(class_df, replace=False, n_samples=desired_size, random_state=42)\n    undersampled_data = pd.concat([undersampled_data, undersampled])","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:18:09.405852Z","iopub.execute_input":"2023-11-19T19:18:09.406251Z","iopub.status.idle":"2023-11-19T19:18:34.074428Z","shell.execute_reply.started":"2023-11-19T19:18:09.406220Z","shell.execute_reply":"2023-11-19T19:18:34.073296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"undersampled_data = undersampled_data.sample(frac=1).reset_index(drop=True)\nprint(undersampled_data.tag.value_counts())\n\nundersampled_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:44:14.954781Z","iopub.execute_input":"2023-11-19T19:44:14.955200Z","iopub.status.idle":"2023-11-19T19:44:14.990089Z","shell.execute_reply.started":"2023-11-19T19:44:14.955168Z","shell.execute_reply":"2023-11-19T19:44:14.989070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Data Processing","metadata":{}},{"cell_type":"markdown","source":"Convert lowercase, remove extra information provided by data source, lemmatize and remove punctuations","metadata":{}},{"cell_type":"code","source":"undersampled_data.lyrics = undersampled_data.lyrics.str.lower()\ndef handleNewLine(text):\n    new_line_idxs = [match.start() for match in re.finditer(r'\\n', text)]\n    \n    lines = []\n    for idx in range(0, len(new_line_idxs)-1):\n        startIndex = new_line_idxs[idx]\n        endIndex = new_line_idxs[idx+1]\n        line = text[startIndex:endIndex]\n        line = line.split(\"\\n\")[1]\n        \n        if len(line)>0:\n            lines.append(line)\n    \n    return ' \\n '.join(lines)\n\nundersampled_data['lyrics'] = undersampled_data['lyrics'].apply(handleNewLine)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:44:17.622972Z","iopub.execute_input":"2023-11-19T19:44:17.623351Z","iopub.status.idle":"2023-11-19T19:44:19.721520Z","shell.execute_reply.started":"2023-11-19T19:44:17.623322Z","shell.execute_reply":"2023-11-19T19:44:19.720494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove extra notes\nundersampled_data['lyrics'] = undersampled_data['lyrics'].str.replace(r'\\[.*?\\]', '', regex=True)\nundersampled_data['lyrics'] = undersampled_data['lyrics'].str.replace(r'\\([^)]*\\)', '', regex=True)\n\n# Remove punctuations from the lyrics column except new line (\\n)\npunctuation = re.compile(r'[^\\w\\s\\n]+')\nundersampled_data['lyrics'] = undersampled_data['lyrics'].apply(lambda x: punctuation.sub('', x).strip())\nundersampled_data['lyrics'] = undersampled_data['lyrics'].str.replace(\" \\n  \\n \", \" \\n \")\n\nrandom_song = np.random.randint(0, len(undersampled_data))\nundersampled_data.lyrics.iloc[random_song], undersampled_data.tag.iloc[random_song]","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:44:19.724559Z","iopub.execute_input":"2023-11-19T19:44:19.724923Z","iopub.status.idle":"2023-11-19T19:44:23.780450Z","shell.execute_reply.started":"2023-11-19T19:44:19.724892Z","shell.execute_reply":"2023-11-19T19:44:23.779449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\nfor index, row in tqdm(undersampled_data.iterrows()):\n    text = row['lyrics']\n    lemmatized_text = []\n    \n    # Lemmatize the text\n    for word in text.split(\" \"):\n        if word not in stop_words or word == \"\\n\":\n            if word == \"\\n\":\n                lemmatized_text.append(word)\n            else:\n                lemmatized_text.append(lemmatizer.lemmatize(word))\n    \n    txt = ' '.join(lemmatized_text)\n    undersampled_data.loc[index, 'lyrics'] = \" \\n \" + txt","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:44:23.782599Z","iopub.execute_input":"2023-11-19T19:44:23.783047Z","iopub.status.idle":"2023-11-19T19:45:13.050893Z","shell.execute_reply.started":"2023-11-19T19:44:23.783007Z","shell.execute_reply":"2023-11-19T19:45:13.049793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"undersampled_data.reset_index(inplace=True, drop=True)\nundersampled_data.lyrics.iloc[random_song], undersampled_data.tag.iloc[random_song]","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:45:13.052033Z","iopub.execute_input":"2023-11-19T19:45:13.052462Z","iopub.status.idle":"2023-11-19T19:45:13.060533Z","shell.execute_reply.started":"2023-11-19T19:45:13.052421Z","shell.execute_reply":"2023-11-19T19:45:13.059503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split each line","metadata":{}},{"cell_type":"code","source":"def split_lyrics(lyric):\n  \"\"\"Splits a lyric into a list of lines.\"\"\"\n  lines = []\n  try:\n      splt = lyric.split(\"\\n\")\n      for line in splt:\n        line = line.strip()\n\n        if len(line) > 1:\n          lines.append(line)\n\n      return lines\n  except:\n        return None\n\nundersampled_data[\"lines\"] = undersampled_data.lyrics.apply(split_lyrics)\nundersampled_data.reset_index(inplace=True, drop=True)\n\nundersampled_data.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:45:13.062984Z","iopub.execute_input":"2023-11-19T19:45:13.063354Z","iopub.status.idle":"2023-11-19T19:45:13.681294Z","shell.execute_reply.started":"2023-11-19T19:45:13.063316Z","shell.execute_reply":"2023-11-19T19:45:13.680203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Tokens","metadata":{}},{"cell_type":"code","source":"def split_tokens(lines):\n    tokens = []\n    \n    try:\n        for line in lines:\n            tokens.append(word_tokenize(line))\n\n        return tokens\n    except:\n        return None\n\nundersampled_data[\"tokens\"] = undersampled_data.lines.apply(split_tokens)\nundersampled_data.reset_index(inplace=True, drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-19T19:45:13.683140Z","iopub.execute_input":"2023-11-19T19:45:13.683582Z","iopub.status.idle":"2023-11-19T19:47:29.089211Z","shell.execute_reply.started":"2023-11-19T19:45:13.683546Z","shell.execute_reply":"2023-11-19T19:47:29.088176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the file","metadata":{}},{"cell_type":"code","source":"undersampled_data.to_csv(\"lyrics_data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HAN Model","metadata":{}},{"cell_type":"code","source":"!pip install torchtext","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:14:50.898152Z","iopub.execute_input":"2023-11-20T10:14:50.898607Z","iopub.status.idle":"2023-11-20T10:15:08.614139Z","shell.execute_reply.started":"2023-11-20T10:14:50.898574Z","shell.execute_reply":"2023-11-20T10:15:08.612359Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchtext in /opt/conda/lib/python3.10/site-packages (0.15.1+cpu)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext) (4.66.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.31.0)\nRequirement already satisfied: torch==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.0.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext) (1.24.3)\nRequirement already satisfied: torchdata==0.6.0 in /opt/conda/lib/python3.10/site-packages (from torchtext) (0.6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->torchtext) (3.1.2)\nRequirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.6.0->torchtext) (1.26.15)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.0->torchtext) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0->torchtext) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.nn.functional as F\n\nfrom torchtext.vocab import GloVe\nimport torchtext\nfrom torchtext.data import get_tokenizer\nfrom torchtext import data\nfrom torchtext import vocab\nfrom nltk.stem.porter import PorterStemmer\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nimport spacy\nspacy_en = spacy.load('en_core_web_sm')\n\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:15:08.617529Z","iopub.execute_input":"2023-11-20T10:15:08.618040Z","iopub.status.idle":"2023-11-20T10:15:10.053957Z","shell.execute_reply.started":"2023-11-20T10:15:08.617997Z","shell.execute_reply":"2023-11-20T10:15:10.052574Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/lyrics/lyrics_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:15:10.055852Z","iopub.execute_input":"2023-11-20T10:15:10.056258Z","iopub.status.idle":"2023-11-20T10:15:11.818412Z","shell.execute_reply.started":"2023-11-20T10:15:10.056224Z","shell.execute_reply":"2023-11-20T10:15:11.816736Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"def tokenize_document(doc):\n    txt = doc.replace('\"', '').replace(\"'\", \"\").replace(\"\\n\", \"\") \n    txt = re.sub(r'\\s+', ' ', txt).strip()\n    \n    return txt.split()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T08:46:10.350460Z","iopub.execute_input":"2023-11-20T08:46:10.350867Z","iopub.status.idle":"2023-11-20T08:46:10.358335Z","shell.execute_reply.started":"2023-11-20T08:46:10.350836Z","shell.execute_reply":"2023-11-20T08:46:10.357189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WordAttention(nn.Module):\n  def __init__(self, hidden_size, embedding_dim):\n    super().__init__()\n\n    self.lin1 = nn.Linear(hidden_size, hidden_size)\n    self.lin2 = nn.Linear(hidden_size, 1, bias=False)\n\n  def forward(self, x):\n    u = torch.tanh(self.lin1(x))\n    attention = F.softmax(self.lin2(x), dim=1)\n\n    output = torch.sum(\n        attention * x, dim=1\n    )\n\n    return attention, output\n\nclass SentenceAttention(nn.Module):\n  def __init__(self, hidden_size, embedding_dim):\n    super().__init__()\n\n    self.lin1 = nn.Linear(hidden_size, hidden_size)\n    self.lin2 = nn.Linear(hidden_size, 1, bias=False)\n\n  def forward(self, x):\n    u = torch.tanh(self.lin1(x))\n    attention = F.softmax(self.lin2(x), dim=1)\n\n    output = torch.sum(\n        attention * x, dim=1\n    )\n\n    return attention, output\n\nclass WordEncoder(nn.Module):\n  def __init__(self, corpus_size, embedding_dim, hidden_size, load_embed=False, weights_matrix=None, trainable_embedding=False):\n    super().__init__()\n\n    self.embedding = nn.Embedding(corpus_size, embedding_dim)\n\n    if load_embed and weights_matrix is not None:\n      self.embedding.load_state_dict({'weight': torch.tensor(weights_matrix)})\n\n    self.embedding.weight.requires_grad = trainable_embedding\n\n    self.gru = nn.GRU(embedding_dim, hidden_size, 2, dropout=0.3, bidirectional=True, batch_first=True)\n    self.attention = WordAttention(hidden_size*2, embedding_dim)\n\n  def forward(self, x):\n    embeddings = self.embedding(x)\n    out, hidden = self.gru(embeddings)\n    attention, out = self.attention(out)\n\n    return out\n\nclass HAN(nn.Module):\n  def __init__(self, corpus_size, embedding_dim, hidden_size, class_count, load_embed=False, weights_matrix=None, trainable_embedding=False):\n    super().__init__()\n    self.class_count = class_count\n    \n    self.wordEncoder = WordEncoder(corpus_size=corpus_size, embedding_dim=embedding_dim, hidden_size=50, load_embed=True, weights_matrix=weights_matrix, trainable_embedding=True)\n\n    self.sentGRU = nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n    self.sentence_attention = SentenceAttention(hidden_size * 2, hidden_size)\n\n    self.softmax = nn.Softmax()\n    self.classifier = nn.Linear(hidden_size*2, self.class_count)\n\n  def forward(self, x):\n    word_output = self.wordEncoder(x) # 16, 100\n\n    sent_out, _ = self.sentGRU(word_output.unsqueeze(1)) # 16, 100\n    _, sent_output = self.sentence_attention(sent_out)\n\n    return self.softmax(self.classifier(sent_output))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:15:11.825900Z","iopub.execute_input":"2023-11-20T10:15:11.827002Z","iopub.status.idle":"2023-11-20T10:15:11.852209Z","shell.execute_reply.started":"2023-11-20T10:15:11.826961Z","shell.execute_reply":"2023-11-20T10:15:11.851203Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 100\nglobal_vectors = GloVe(name='6B', dim=embedding_dim) # 42B, 840B","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:15:11.854200Z","iopub.execute_input":"2023-11-20T10:15:11.855155Z","iopub.status.idle":"2023-11-20T10:15:12.782486Z","shell.execute_reply.started":"2023-11-20T10:15:11.855045Z","shell.execute_reply":"2023-11-20T10:15:12.780560Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"unique_words = list()\nstemmer = PorterStemmer()\nX = []\nfor lyric in tqdm(list(dataset.lyrics)):\n    lyric = lyric.replace(\" \\n \", \" \").strip()\n    token_list = lyric.split()\n    \n    filtered_tokens = []\n    for token in token_list:\n        token = stemmer.stem(token)\n        filtered_tokens.append(token)\n        \n        if not token in unique_words:\n            unique_words.append(token)\n    X.append(' '.join(filtered_tokens))\n    \nunique_words = list(unique_words)\nprint(\"Corpus size:\", len(unique_words))","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:15:12.785210Z","iopub.execute_input":"2023-11-20T10:15:12.785880Z","iopub.status.idle":"2023-11-20T10:32:42.455724Z","shell.execute_reply.started":"2023-11-20T10:15:12.785821Z","shell.execute_reply":"2023-11-20T10:32:42.453922Z"},"trusted":true},"execution_count":118,"outputs":[{"name":"stderr","text":"100%|██████████| 21258/21258 [17:29<00:00, 20.25it/s] ","output_type":"stream"},{"name":"stdout","text":"Corpus size: 105705\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_words[:3]","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:32:42.459014Z","iopub.execute_input":"2023-11-20T10:32:42.459640Z","iopub.status.idle":"2023-11-20T10:32:42.469443Z","shell.execute_reply.started":"2023-11-20T10:32:42.459586Z","shell.execute_reply":"2023-11-20T10:32:42.468149Z"},"trusted":true},"execution_count":119,"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"['shall', 'absurd', 'heart']"},"metadata":{}}]},{"cell_type":"code","source":"corpus_size = len(unique_words)\nweights_matrix = np.zeros((corpus_size, embedding_dim))\n\nfound_word = 0\nfor i, word in enumerate(unique_words):\n  word_vector = global_vectors.get_vecs_by_tokens(word)\n\n  if word_vector.sum().item() == '0':\n    weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n  else:\n    weights_matrix[i] = word_vector\n    found_word += 1\n    \nprint(found_word)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:32:55.087581Z","iopub.execute_input":"2023-11-20T10:32:55.088130Z","iopub.status.idle":"2023-11-20T10:32:59.987160Z","shell.execute_reply.started":"2023-11-20T10:32:55.088076Z","shell.execute_reply":"2023-11-20T10:32:59.985770Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"105705\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\ny = enc.fit_transform(np.array(dataset.tag).reshape(-1, 1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:33:06.067486Z","iopub.execute_input":"2023-11-20T10:33:06.068027Z","iopub.status.idle":"2023-11-20T10:33:06.088306Z","shell.execute_reply.started":"2023-11-20T10:33:06.067989Z","shell.execute_reply":"2023-11-20T10:33:06.086542Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, X, y, unique_words, weights_matrix):\n        self.X = X\n        self.y = y\n        self.unique_words = unique_words\n        self.weights_matrix = weights_matrix\n        \n        self.stemmer = PorterStemmer()\n        \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        sentence = self.X[idx]\n        label = self.y[idx]\n\n        indices = [self.unique_words.index(word)+1 for word in sentence.split()]\n\n        return {\n            'input': torch.tensor(indices, dtype=torch.long),\n            'label': torch.tensor(label, dtype=torch.float)\n        }\n\ndef collate_fn(batch):\n    inputs = [item['input'] for item in batch]\n    labels = [item['label'] for item in batch]\n\n    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n\n    return {\n        'input': inputs_padded,\n        'label': torch.stack(labels)\n    }","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:33:07.823984Z","iopub.execute_input":"2023-11-20T10:33:07.824551Z","iopub.status.idle":"2023-11-20T10:33:07.838089Z","shell.execute_reply.started":"2023-11-20T10:33:07.824512Z","shell.execute_reply":"2023-11-20T10:33:07.836273Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\n\ncustom_dataset = CustomDataset(X=X, y=y, unique_words=unique_words, weights_matrix=weights_matrix)\ntrain_size = int(0.75 * len(custom_dataset))\ntest_size = len(custom_dataset) - train_size\n\ntrain_dataset, test_dataset = random_split(custom_dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\ntest_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:36:38.790549Z","iopub.execute_input":"2023-11-20T10:36:38.791028Z","iopub.status.idle":"2023-11-20T10:36:38.801015Z","shell.execute_reply.started":"2023-11-20T10:36:38.790991Z","shell.execute_reply":"2023-11-20T10:36:38.799879Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"def train(model, optim, loss_fn, epochs=50, print_loss=False):\n  for epoch in range(50):\n    epoch_loss = 0\n\n    model.train()\n    for i in tqdm(train_loader):\n      optim.zero_grad()\n\n      output = model(i[\"input\"])\n      target = i[\"label\"]\n      # target = target.unsqueeze(1)\n\n      loss = loss_fn(target, output)\n      epoch_loss += loss.item()\n\n      loss.backward()\n      optim.step()\n\n    model.eval()\n    for i in test_loader:\n      output = model(i[\"input\"])\n      target = i[\"label\"]\n      # target = target.unsqueeze(1)\n\n      l = loss_fn(target, output)\n\n    if print_loss:\n      if epoch % 5 == 0:\n        print(\"Epoch loss:\", round(epoch_loss/len(train_loader), 4))\n        print(\"Eval Loss:\", round(l.item(), 4))\n\n  print(\"Eval Loss:\", round(l.item(), 4))\n  return model","metadata":{"execution":{"iopub.status.busy":"2023-11-20T10:36:46.853599Z","iopub.execute_input":"2023-11-20T10:36:46.854022Z","iopub.status.idle":"2023-11-20T10:36:46.868998Z","shell.execute_reply.started":"2023-11-20T10:36:46.853987Z","shell.execute_reply":"2023-11-20T10:36:46.867897Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"corpus_size, embedding_dim = weights_matrix.shape\n\nhan_model = HAN(corpus_size=corpus_size, embedding_dim=embedding_dim, hidden_size=10, class_count=len(set(dataset.tag)),\n                load_embed=True, weights_matrix=weights_matrix, trainable_embedding=False)\noptim =  torch.optim.Adam(han_model.parameters(), 0.001)\nloss_fn = torch.nn.CrossEntropyLoss()\nhan_model = train(han_model, optim, loss_fn, epochs=10, print_loss=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}